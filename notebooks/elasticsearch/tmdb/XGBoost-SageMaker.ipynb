{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Ranklib is a relatively old library and doesn't have the wide spread use that XGBoost does. Ranklib is still under active development, but the fork of the project OSC created reflects an older version.\n",
    "\n",
    "The ES-LTR plugin is designed to work with XGBoost model format. This notebook starts with the `classic` training data generated in `hello-ltr.py` and shows how you could use XGBoost instead of Ranklib to create a model and use it with the plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "bucket_name = sagemaker.Session().default_bucket()\n",
    "bucket_prefix = \"es-ltr-xgboost\"\n",
    "sgmk_session = sagemaker.Session()\n",
    "sgmk_client = boto_session.client(\"sagemaker\")\n",
    "sgmk_role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121021644041.dkr.ecr.ap-southeast-1.amazonaws.com/sagemaker-xgboost:1.2-1\n"
     ]
    }
   ],
   "source": [
    "training_image = sagemaker.image_uris.retrieve(\n",
    "    \"xgboost\", region=region, version=\"1.2-1\"\n",
    ")\n",
    "\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data\n",
    "\n",
    "Gather the data generated for our `classic` model in `hello-ltr.ipynb`. If this file doesn't exist yet, rerun that notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognizing 1 queries in: data/classic-training.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features0</th>\n",
       "      <th>uid</th>\n",
       "      <th>qid</th>\n",
       "      <th>keywords</th>\n",
       "      <th>docId</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014.0</td>\n",
       "      <td>1_374430</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>374430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1995.0</td>\n",
       "      <td>1_19404</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>19404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1994.0</td>\n",
       "      <td>1_278</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>1_372058</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>372058</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1972.0</td>\n",
       "      <td>1_238</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>238</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>1_177699</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>177699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2011.0</td>\n",
       "      <td>1_62835</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>62835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2008.0</td>\n",
       "      <td>1_4944</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>4944</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1997.0</td>\n",
       "      <td>1_9404</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>9404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1991.0</td>\n",
       "      <td>1_9319</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>9319</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     features0       uid  qid keywords   docId  grade\n",
       "0       2014.0  1_374430    1           374430      0\n",
       "1       1995.0   1_19404    1            19404      1\n",
       "2       1994.0     1_278    1              278      1\n",
       "3       2016.0  1_372058    1           372058      0\n",
       "4       1972.0     1_238    1              238      2\n",
       "..         ...       ...  ...      ...     ...    ...\n",
       "995     2013.0  1_177699    1           177699      0\n",
       "996     2011.0   1_62835    1            62835      0\n",
       "997     2008.0    1_4944    1             4944      1\n",
       "998     1997.0    1_9404    1             9404      1\n",
       "999     1991.0    1_9319    1             9319      1\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ltr.judgments as judge\n",
    "df = [j for j in judge.judgments_from_file(open('data/classic-training.txt'))]\n",
    "df = judge.judgments_to_dataframe(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries for xgboost-ing\n",
    "\n",
    "Just the dependencies we need to train and visualize out model trained with XG-Boost instead of Ranklib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 50,150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up our training Matrix\n",
    "\n",
    "XGBoost has it's data specficiations so we need to get out features into that format to use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['grade', 'features0']]\n",
    "features = df[['features0']]\n",
    "labels = df[['grade']]\n",
    "\n",
    "#dmx = xgb.DMatrix(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-ap-southeast-1-344028372807/es-ltr-xgboost/train-sm.csv', 'S3DataDistributionType': 'FullyReplicated'}}, 'ContentType': 'csv'}\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"data/train-sm.csv\", index=False, header=False)\n",
    "train_uri = sgmk_session.upload_data(\n",
    "    path=\"data/train-sm.csv\",\n",
    "    bucket=bucket_name,\n",
    "    key_prefix=bucket_prefix,\n",
    ")\n",
    "\n",
    "# Define the data input channels for the training job:\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(train_uri, content_type=\"csv\")\n",
    "\n",
    "print(f\"{s3_input_train.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the first XGBoost model\n",
    "\n",
    "Using the demo parameters for our model, we will train a standard regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-13 01:58:41 Starting - Starting the training job...\n",
      "2021-10-13 01:59:04 Starting - Launching requested ML instancesProfilerReport-1634090320: InProgress\n",
      "...\n",
      "2021-10-13 01:59:32 Starting - Preparing the instances for training.........\n",
      "2021-10-13 02:01:05 Downloading - Downloading input data...\n",
      "2021-10-13 02:01:25 Training - Downloading the training image...\n",
      "2021-10-13 02:02:10 Uploading - Uploading generated training model\n",
      "2021-10-13 02:02:10 Completed - Training job completed\n",
      "\u001b[34m[2021-10-13 02:01:58.491 ip-10-0-247-209.ap-southeast-1.compute.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value rank:pairwise to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 1000 rows and 1 columns\u001b[0m\n",
      "\u001b[34m[0]#011train-map:1.00000\u001b[0m\n",
      "\u001b[34m[1]#011train-map:1.00000\u001b[0m\n",
      "Training seconds: 65\n",
      "Billable seconds: 18\n",
      "Managed Spot Training savings: 72.3%\n"
     ]
    }
   ],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=training_image,  # XGBoost algorithm container\n",
    "    instance_type=\"ml.m5.xlarge\",  # type of training instance\n",
    "    instance_count=1,  # number of instances to be used\n",
    "    role=sgmk_role,  # IAM role to be used\n",
    "    max_run=20 * 60,  # Maximum allowed active runtime\n",
    "    use_spot_instances=True,  # Use spot instances to reduce cost\n",
    "    max_wait=30 * 60,  # Maximum clock time (including spot delays)\n",
    ")\n",
    "\n",
    "# define its hyperparameters\n",
    "estimator.set_hyperparameters(\n",
    "    num_round=2,  # int: [1,300]\n",
    "    max_depth=2,  # int: [1,10]\n",
    "    eta=1,  # float: [0,1]\n",
    "    objective=\"rank:pairwise\",\n",
    ")\n",
    "\n",
    "# start a training (fitting) job\n",
    "estimator.fit({\"train\": s3_input_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param = {'max_depth':2, 'eta':1, 'silent':1}\n",
    "#num_round = 2\n",
    "\n",
    "#model = xgb.train(param, dmx, num_round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect as dataframe\n",
    "\n",
    "Looking at the model as a dataframe can tell you which splits helped the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trees_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust the objective for LTR\n",
    "\n",
    "Really we don't want the regression as our objective function. In LTR we take advantage of a new pairwise loss function to find the optimal splits for a regression tree. \n",
    "\n",
    "This doesn't make a massive difference for the model that is generated because it is still a regression tree at the end of the day, but we are not longer using residual sqared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param2 = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'rank:pairwise'}\n",
    "\n",
    "#ranking_model = xgb.train(param2, dmx, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_model.trees_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(ranking_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading an XGBoost model to the plugin\n",
    "\n",
    "Since the model can be represented with JSON, the plugin can parse it. But we need to make sure the plugin gets the proper feature value names in order for it to parse properly.\n",
    "\n",
    "These are supplied via a mapping `txt` file, `fmap.txt`.\n",
    "\n",
    "The first step is to dump the model with the feature mapping to the features already stored in the plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dump = ranking_model.get_dump(fmap='fmap.txt', dump_format='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Massage the JSON\n",
    "\n",
    "Manipulate the XGBoost output format to clean it up for posting to the plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "clean_model = []\n",
    "for line in model_dump:\n",
    "    clean_model.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post it to the plugin\n",
    "\n",
    "Still referencing the index and feature set the model will be associated with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ltr.client as client\n",
    "client = client.ElasticClient()\n",
    "\n",
    "client.submit_xgboost_model('release', 'tmdb', 'xgb', clean_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.release_date_plot import search\n",
    "search(client, 'batman', 'xgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare it to the classic Ranklib model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.release_date_plot import plot\n",
    "plot(client, \"batman\", models = ['classic', 'xgb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
